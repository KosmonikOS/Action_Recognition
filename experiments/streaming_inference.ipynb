{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Streaming SODE Inference from YOLO Pose Skeletons\n",
        "\n",
        "This notebook wires the pretrained SODE action recogniser with a YOLOv11 pose estimator. It streams skeleton detections from a video, buffers the last `window_size` frames, and emits action predictions once enough frames are available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Prerequisites**\n",
        "> - Install `ultralytics` (e.g. `pip install ultralytics`).\n",
        "> - Place the YOLOv11 pose weights at `../models/yolo11x-pose.pt` (or update the path below).\n",
        "> - Ensure `experiments/sode_best.pt` is the checkpoint produced by the training notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "66160b81",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from act_rec.model.sode import SODE\n",
        "from act_rec.params import YoloPoseVideoInferenceParams\n",
        "from act_rec.labeling import YoloPoseVideoLabeler\n",
        "from act_rec.preprocessing import preprocess_sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8951713f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps | YOLO device: mps\n",
            "SODE checkpoint: /Users/nikita/University/CV_PMLDL_DR_Project/experiments/sode_best.pt\n",
            "YOLO weights:    /Users/nikita/University/CV_PMLDL_DR_Project/models/yolo11x-pose.pt\n",
            "Input video:     /Users/nikita/University/CV_PMLDL_DR_Project/data/sample2.mp4\n"
          ]
        }
      ],
      "source": [
        "# Paths and runtime configuration\n",
        "window_size = 64  # Number of skeleton frames per SODE window\n",
        "\n",
        "project_root = Path('..').resolve()\n",
        "sode_checkpoint_path = project_root / 'experiments' / 'sode_best.pt'\n",
        "yolo_model_path = project_root / 'models' / 'yolo11x-pose.pt'\n",
        "video_path = project_root / 'data' / 'sample2.mp4'  # <-- update to your video path\n",
        "\n",
        "# Pick the runtime device. The YOLO params expect a string while PyTorch uses torch.device.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    yolo_device = 'cuda:0'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    yolo_device = 'mps'\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    yolo_device = 'cpu'\n",
        "\n",
        "yolo_params = YoloPoseVideoInferenceParams()\n",
        "yolo_params.device = yolo_device\n",
        "\n",
        "print(f'Using device: {device} | YOLO device: {yolo_params.device}')\n",
        "print(f'SODE checkpoint: {sode_checkpoint_path}')\n",
        "print(f'YOLO weights:    {yolo_model_path}')\n",
        "print(f'Input video:     {video_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14fc68fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sode_model(checkpoint_path: Path, device: torch.device, window_size: int = 64):\n",
        "    \"\"\"Load the pretrained SODE checkpoint and expose an index->label mapping.\"\"\"\n",
        "    if not checkpoint_path.exists():\n",
        "        raise FileNotFoundError(f'SODE checkpoint not found at {checkpoint_path}')\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    state_dict = checkpoint.get('model')\n",
        "    if state_dict is None:\n",
        "        raise KeyError(\"Checkpoint must contain a 'model' state dict.\")\n",
        "\n",
        "    label_to_idx = checkpoint.get('label_to_idx')\n",
        "    if label_to_idx is None:\n",
        "        raise KeyError(\"Checkpoint must contain 'label_to_idx' mapping.\")\n",
        "\n",
        "    num_classes = len(label_to_idx)\n",
        "    model = SODE(\n",
        "        num_class=num_classes,\n",
        "        num_point=17,\n",
        "        num_person=1,\n",
        "        graph='act_rec.graph.coco.Graph',\n",
        "        in_channels=3,\n",
        "        T=window_size,\n",
        "        n_step=3,\n",
        "        num_cls=4,\n",
        "    )\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
        "    return model, idx_to_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6deeacc3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded SODE with 14 classes. Example labels: ['bench_press', 'clean_and_jerk', 'handstand_pushups', 'handstand_walking', 'jump_rope']\n"
          ]
        }
      ],
      "source": [
        "sode_model, idx_to_label = load_sode_model(sode_checkpoint_path, device, window_size=window_size)\n",
        "labeler = YoloPoseVideoLabeler(model_path=str(yolo_model_path), params=yolo_params)\n",
        "\n",
        "preview = [idx_to_label[i] for i in sorted(idx_to_label)[:5]]\n",
        "print(f'Loaded SODE with {len(idx_to_label)} classes. Example labels: {preview}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "40003beb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def skeleton_from_result(result):\n",
        "    \"\"\"Extract a single-person skeleton (V, 3) array from a YOLO result.\"\"\"\n",
        "    keypoints = getattr(result, 'keypoints', None)\n",
        "    if keypoints is None:\n",
        "        return None\n",
        "    data = getattr(keypoints, 'data', None)\n",
        "    if data is None or data.shape[0] != 1:\n",
        "        return None\n",
        "    return data[0].cpu().numpy()\n",
        "\n",
        "\n",
        "CONF_THRESHOLD = 0.20\n",
        "\n",
        "\n",
        "def interpolate_linear_1d(y: np.ndarray, mask_valid: np.ndarray) -> np.ndarray:\n",
        "    if mask_valid.all():\n",
        "        return y\n",
        "    if (~mask_valid).all():\n",
        "        return np.zeros_like(y)\n",
        "    x = np.arange(y.shape[0])\n",
        "    xp = x[mask_valid]\n",
        "    fp = y[mask_valid]\n",
        "    return np.interp(x, xp, fp).astype(np.float32)\n",
        "\n",
        "\n",
        "def interpolate_joints_over_time(xy: np.ndarray, conf: np.ndarray, conf_thr: float) -> np.ndarray:\n",
        "    xy_interp = xy.copy()\n",
        "    valid = conf >= conf_thr\n",
        "    T, V, _ = xy.shape\n",
        "    for v in range(V):\n",
        "        mask = valid[:, v]\n",
        "        for c in range(2):\n",
        "            xy_interp[:, v, c] = interpolate_linear_1d(xy[:, v, c], mask)\n",
        "    return xy_interp\n",
        "\n",
        "\n",
        "def center_and_scale_sequence(joints_xy: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
        "    left_hip, right_hip = 11, 12\n",
        "    left_sh, right_sh = 5, 6\n",
        "    pelvis = (joints_xy[:, left_hip, :] + joints_xy[:, right_hip, :]) / 2.0\n",
        "    pelvis_valid = np.isfinite(pelvis).all(axis=1)\n",
        "    if pelvis_valid.any():\n",
        "        pelvis_ref = pelvis[pelvis_valid][0]\n",
        "    else:\n",
        "        pelvis_ref = np.zeros(2, dtype=np.float32)\n",
        "    spine = (joints_xy[:, left_sh, :] + joints_xy[:, right_sh, :]) / 2.0\n",
        "    torso_vec = spine - pelvis\n",
        "    torso_len = np.linalg.norm(torso_vec, axis=1)\n",
        "    torso_valid = torso_len > eps\n",
        "    if torso_valid.any():\n",
        "        scale = float(np.median(torso_len[torso_valid]))\n",
        "    else:\n",
        "        scale = 1.0\n",
        "    scale = max(scale, eps)\n",
        "    joints_centered = joints_xy - pelvis_ref[None, None, :]\n",
        "    joints_cs = joints_centered / scale\n",
        "    return joints_cs.astype(np.float32)\n",
        "\n",
        "\n",
        "def normalize_skeleton_sequence(sequence: np.ndarray, conf_thr: float) -> np.ndarray:\n",
        "    seq_np = np.asarray(sequence, dtype=np.float32)\n",
        "    xy = seq_np[:, :, :2]\n",
        "    conf = seq_np[:, :, 2]\n",
        "    xy = interpolate_joints_over_time(xy, conf, conf_thr)\n",
        "    joints_cs = center_and_scale_sequence(xy)\n",
        "    return np.concatenate([joints_cs, conf[:, :, None]], axis=2).astype(np.float32)\n",
        "\n",
        "\n",
        "def preprocess_for_sode(window: np.ndarray, window_size: int, conf_thr: float) -> torch.Tensor:\n",
        "    normalized = normalize_skeleton_sequence(window, conf_thr)\n",
        "    data_tensor, _ = preprocess_sequence(\n",
        "        normalized,\n",
        "        window_size=window_size,\n",
        "        p_interval=(1.0,),\n",
        "        random_rotation=False,\n",
        "        use_velocity=False,\n",
        "        valid_frame_num=normalized.shape[0],\n",
        "    )\n",
        "    return data_tensor\n",
        "\n",
        "\n",
        "def predict_from_window(model: SODE, window_np: np.ndarray, device: torch.device, window_size: int, conf_thr: float) -> np.ndarray:\n",
        "    data_tensor = preprocess_for_sode(window_np, window_size, conf_thr)\n",
        "    inputs = data_tensor.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits, *_ = model(inputs)\n",
        "        scores = logits[:, :, -1]\n",
        "        probs = torch.softmax(scores, dim=-1)\n",
        "    return probs.squeeze(0).cpu().numpy()\n",
        "\n",
        "\n",
        "def stream_video_predictions(\n",
        "    video_path: Path,\n",
        "    model: SODE,\n",
        "    idx_to_label: dict[int, str],\n",
        "    labeler: YoloPoseVideoLabeler,\n",
        "    *,\n",
        "    window_size: int = 64,\n",
        "    multi_person_tolerance: int = 15,\n",
        ") -> list[dict[str, float | str | int]]:\n",
        "    \"\"\"Stream a video, accumulate skeletons, and emit predictions per window.\n",
        "\n",
        "    If total video is shorter than window_size, make a single prediction from the buffer.\"\"\"\n",
        "    if not video_path.exists():\n",
        "        raise FileNotFoundError(f'Video not found at {video_path}')\n",
        "\n",
        "    buffer: deque[np.ndarray] = deque(maxlen=window_size)\n",
        "    predictions: list[dict[str, float | str | int]] = []\n",
        "    skip_counter = 0\n",
        "    start_ts = time.perf_counter()\n",
        "    frame_count = 0\n",
        "\n",
        "    stream = labeler.model(\n",
        "        str(video_path),\n",
        "        stream=True,\n",
        "        device=labeler.params.device,\n",
        "        imgsz=labeler.params.imgsz,\n",
        "        rect=labeler.params.rect,\n",
        "        batch=labeler.params.batch,\n",
        "        vid_stride=labeler.params.vid_stride,\n",
        "        verbose=labeler.params.verbose,\n",
        "    )\n",
        "\n",
        "    for frame_idx, result in enumerate(stream):\n",
        "        skeleton = skeleton_from_result(result)\n",
        "        if skeleton is None:\n",
        "            skip_counter += 1\n",
        "            if skip_counter >= multi_person_tolerance:\n",
        "                buffer.clear()\n",
        "            continue\n",
        "\n",
        "        skip_counter = 0\n",
        "        buffer.append(skeleton)\n",
        "        frame_count += 1\n",
        "\n",
        "        if len(buffer) < window_size:\n",
        "            continue\n",
        "\n",
        "        window_np = np.stack(buffer, axis=0)\n",
        "        probs = predict_from_window(model, window_np, device, window_size, CONF_THRESHOLD)\n",
        "        pred_idx = int(probs.argmax())\n",
        "        pred_label = idx_to_label.get(pred_idx, str(pred_idx))\n",
        "        pred_conf = float(probs[pred_idx])\n",
        "        frame_tag = str(frame_idx).zfill(4)\n",
        "\n",
        "        record = {\n",
        "            'frame_idx': frame_idx,\n",
        "            'elapsed_s': time.perf_counter() - start_ts,\n",
        "            'prediction': pred_label,\n",
        "            'confidence': pred_conf,\n",
        "        }\n",
        "        predictions.append(record)\n",
        "        print(f\"[frame {frame_tag}] {pred_label} (p={pred_conf:.2f})\")\n",
        "\n",
        "    if frame_count > 0 and len(predictions) == 0 and len(buffer) > 0:\n",
        "        window_np = np.stack(buffer, axis=0)\n",
        "        probs = predict_from_window(model, window_np, device, window_size, CONF_THRESHOLD)\n",
        "        pred_idx = int(probs.argmax())\n",
        "        pred_label = idx_to_label.get(pred_idx, str(pred_idx))\n",
        "        pred_conf = float(probs[pred_idx])\n",
        "        frame_tag = str(frame_count - 1).zfill(4)\n",
        "        record = {\n",
        "            'frame_idx': frame_count - 1,\n",
        "            'elapsed_s': time.perf_counter() - start_ts,\n",
        "            'prediction': pred_label,\n",
        "            'confidence': pred_conf,\n",
        "        }\n",
        "        predictions.append(record)\n",
        "        print(f\"[frame {frame_tag}] {pred_label} (p={pred_conf:.2f})\")\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2770bcf1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[frame 0054] squat (p=0.93)\n"
          ]
        }
      ],
      "source": [
        "# Run the streaming inference.\n",
        "# Update `video_path` above before executing this cell.\n",
        "predictions = stream_video_predictions(\n",
        "    video_path,\n",
        "    sode_model,\n",
        "    idx_to_label,\n",
        "    labeler,\n",
        "    window_size=window_size,\n",
        "    multi_person_tolerance=15,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5f07b6e6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>frame_idx</th>\n",
              "      <th>elapsed_s</th>\n",
              "      <th>prediction</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>54</td>\n",
              "      <td>1.526039</td>\n",
              "      <td>squat</td>\n",
              "      <td>0.929044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   frame_idx  elapsed_s prediction  confidence\n",
              "0         54   1.526039      squat    0.929044"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Inspect the collected predictions (requires pandas for pretty display).\n",
        "if predictions:\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        display(pd.DataFrame(predictions))\n",
        "    except ImportError:\n",
        "        print(predictions)\n",
        "else:\n",
        "    print('No predictions were produced. Check the video path and detections.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb7d43f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "act-rec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
